---
title: "grafici-commento"
output: html_document
date: '2023-03-03'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SIZE SCALING

```{r, echo=F, message=F}
library(ggplot2)
data <- read.delim2("DATA-HPC.dat")
```


```{r, echo=FALSE}
subset01 <- data[c(1:19, 39:57),]
plot01 <- ggplot(subset01, aes(x=DIM,y=GFLOPS, color=LIBRARY)) + geom_line() + geom_point()
plot01 + ggtitle("Comparison OpenBLAS and MKL on EPYC + Single Precision + SPREAD allocation policy") +
  xlab("Matrix Size") + ylab("GFLOPS") + scale_y_continuous(breaks=seq(0,3000,200)) + scale_x_continuous(breaks=seq(0,20000,2000))
```
On an epyc node, SPREAD policy in single precision shows different trends with respect to the considered library.
MKL seems to perform better with small matrix sizes: it has a peak in correspondence to $size=7.000$, with $2600\;GFLOPS$. After that, MKL performances decrease and change only in correspondence to $size=18.000$, where we observe less than $1.700\;GFLOPS$.
On the other hand, for OpenBLAS the number of $GFLOPS$ grows accordingly with the matrix size. 

At a size of $12.000$, both OpenBLAS and MKL seem to perform equivalently. 
Another thing that is in common between these 2 trends is that we observe a growth in terms of $GFLOPS$ both for OpenBLAS and MKL for matrix sizes equal to $18.000$.

### Matrix with elements expressed in double precision

```{r, echo=FALSE}
subset02 <- data[c(20:38, 58:76),]
plot02 <- ggplot(subset02, aes(x=DIM,y=GFLOPS, color=LIBRARY)) + geom_line() + geom_point()
plot02 + ggtitle("Comparison OpenBLAS and MKL on EPYC + Double Precision + SPREAD allocation policy") +
  xlab("Matrix Size") + ylab("GFLOPS") + scale_y_continuous(breaks=seq(0,3000,200)) + scale_x_continuous(breaks=seq(0,20000,2000))
```

If we now consider a double precision matrix representation, we can spot many differences. 
First of all, the overall maximum in terms of $GFLOPS$ ($1.100$) is less than an half the overall maximum that we can observe if we use a SPREAD policy allocation ($2.600$). 
Then, it's easy to see that MKL still shows 2 peaks, but they can be found in correspondence to different matrix sizes than before.
In double precision, both OpenBLAS and MKL show almost the same trend, and MKL is, almost always, the more performant. 
The peak performance can be observed by using OpenBLAS. 

To sum up, what emerges from these two analysis is that the expression of matrix elements in double precision almost halves the performance (i.e. the number of $GFLOPS$) that we can reach. If we manage to use single precision numbers, we can state that if matrix size is below $12.000$ it's more convenient to use MKL in order to have higher performences; otherwise, OpenBLAS would be the best option (at least until a matrix size of $20.000$). 


### A different binding policy: CLOSE

```{r}
subset03 <- data[c(77:95, 115:133),]
plot03 <- ggplot(subset03, aes(x=DIM,y=GFLOPS, color=LIBRARY)) + geom_line() + geom_point()
plot03 + ggtitle("Comparison OpenBLAS and MKL on EPYC + Single Precision + CLOSE allocation policy") +
  xlab("Matrix Size") + ylab("GFLOPS") + scale_y_continuous(breaks=seq(0,3000,200)) + scale_x_continuous(breaks=seq(0,20000,2000))
```
We try to change the binding policy in order to study whether we could obtain better performances in this case.
This graph highlight that now MKL is better/slightly better than OpenBLAS only for matrices of sizes between $3.000$ and $\sim 6.500$; for all other matrix sizes, OpenBLAS apparently outperforms MKL. 
Despite this, the maximum observed value of $GFLOPS$ is $2.400$, while for a SPREAD policy allocation it was greater. What changes in this situation is that now the OpenBLAS performance trend seems to grow accordingly to the matrix size (which is really good), while before we had only a high value for relatively small matrices. 
Up to what we can observe, CLOSE appears to be a more reasonable allocation policy accordingly to its ability to better scale if we increase the matrix size. 

*Il calo nelle performances è perché per matrici di grandezza 5000 usciamo dalla L1?*
*5000x4bytesx4bytes = 80.000 bytes = 80 KB? Ma la L1 non era di 256 KB?*


```{r}
subset04 <- data[c(96:114, 134:152),]
plot04 <- ggplot(subset04, aes(x=DIM,y=GFLOPS, color=LIBRARY)) + geom_line() + geom_point()
plot04 + ggtitle("Comparison OpenBLAS and MKL on EPYC + Double Precision + CLOSE allocation policy") +
  xlab("Matrix Size") + ylab("GFLOPS") + scale_y_continuous(breaks=seq(0,3000,200)) + scale_x_continuous(breaks=seq(0,20000,1000))
```

*Lo commentiamo o lasciamo stare? Secondo me l'importante è la Single Precision*

```{r}
subset05 <- data[c(1:38, 77:114),]

plot05 <- ggplot(subset05, aes(x=DIM,y=GFLOPS, color=SYNTH)) + geom_line() + geom_point()
plot05 + ggtitle("OpenBLAS comparisons: different precisions and allocation policies") +
  xlab("Matrix Size") + ylab("GFLOPS") + scale_y_continuous(breaks=seq(0,3000,200)) + scale_x_continuous(breaks=seq(0,20000,2000))

```



```{r}
subset06 <- data[c(39:76, 115:152),]

plot06 <- ggplot(subset06, aes(x=DIM,y=GFLOPS, color=SYNTH)) + geom_line() + geom_point()
plot06 + ggtitle("MKL comparisons: different precisions and allocation policies") +
  xlab("Matrix Size") + ylab("GFLOPS") + scale_y_continuous(breaks=seq(0,3000,200)) + scale_x_continuous(breaks=seq(0,20000,2000))

```




